{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730672ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\programdata\\anaconda3\\lib\\site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afa62ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata-01\n",
      "Metadata-02\n",
      "Metadata-03\n",
      "Metadata-04\n",
      "Metadata-05\n",
      "Metadata-06\n",
      "Metadata-07\n",
      "Metadata-08\n",
      "Metadata-09\n",
      "Metadata-10\n",
      "Metadata-11\n",
      "Metadata-12\n",
      "Metadata-13\n",
      "Metadata-14\n",
      "Metadata-15\n",
      "Metadata-16\n",
      "Metadata-17\n"
     ]
    }
   ],
   "source": [
    "#download the data from the sdg website, they have a button that allows u to download all\n",
    "#take the file u get and put it inside the same directory as the jupyter notebook file\n",
    "#create an empty file named \"corpus\"\n",
    "#at the end you should get a corpus file containing 17 word documents, each for a specific SDG\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "\n",
    "for i in range(1,18):\n",
    "    if(i<10):\n",
    "        fileName = \"Metadata-0\" +str(i)\n",
    "    else:\n",
    "        fileName = \"Metadata-\" +str(i)\n",
    "    print(fileName)\n",
    "    initDoc = Document()\n",
    "    for file in os.listdir(\"./SDG-indicator-metadata\"):\n",
    "        if file.startswith(fileName) and file.endswith(\".docx\"):  \n",
    "            source_document = Document(\"./SDG-indicator-metadata/\"+ file)\n",
    "            for paragraph in source_document.paragraphs:\n",
    "                text = paragraph.text\n",
    "                initDoc.add_paragraph(text)\n",
    "    initDoc.save(\"./corpus/SDG\" + str(i) + \".docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d3720b",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "we will create some functions that we can use in order to make sure our data is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d040d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "def decontract_words(text):\n",
    "# creating an empty list\n",
    "    expanded_words = []\n",
    "    for word in text.split():\n",
    "# using contractions.fix to expand the shortened words\n",
    "      expanded_words.append(contractions.fix(word))\n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2e1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the methods we will use\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "UrlRegex = r'https?://\\S+|www.\\S+'\n",
    "HtmlStyleLinkRegex = r'<a\\s+(?:[^>]*?\\s+)?href=([\"])(.*?)\\1'\n",
    "ampRegex = r'&amp;'\n",
    "brRegex = '<br/> | <br> | </br>'\n",
    "specialCharRegex = r'[^a-zA-Z0-9-\" \"]'\n",
    "\n",
    "\n",
    "def filterRegex(s):\n",
    "    \n",
    "    #remove all html style link\n",
    "    br_regex = re.compile(r'<(?!br).*?>')\n",
    "    filtered_text = br_regex.sub('', s)\n",
    "    filtered_text = re.sub(UrlRegex,'', filtered_text)\n",
    "    filtered_text = re.sub(ampRegex, '', filtered_text)\n",
    "    \n",
    "    # remove all <br>\n",
    "    filtered_text = re.sub(brRegex, '', filtered_text)\n",
    "\n",
    "    \n",
    "    # remove all special characters\n",
    "    filtered_text = re.sub(specialCharRegex,'',filtered_text)\n",
    "    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebc054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    words = s.split()\n",
    "    newWords= []\n",
    "    for val in words:\n",
    "        if (val not in stops):\n",
    "            newWords.append(val)\n",
    "    \n",
    "    return \" \".join(newWords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a326c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#suppose we have nonsense written in our document, they will be removed by checking if thr word belongs to nltk.corpus.words\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_nonsense(sent): \n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf64ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you look inside the document, we have enumerations using letters and numbers, for ex a0, a1 etc... so we used this function \n",
    "#to remove the digits from them and we then used the function remove_alone_char to remove remaining letters that are alone in the context\n",
    "# for ex a, b, c , ...\n",
    "def removeDigitsFromString(s):\n",
    "    words = s.split()\n",
    "    fixedWords = []\n",
    "    for val in words:\n",
    "        f=filter(str.isalpha,val)\n",
    "        s1= \"\".join(f)\n",
    "        fixedWords.append(s1)\n",
    "    \n",
    "    return \" \".join(fixedWords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f88e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_alone_char(s):\n",
    "    words = s.split()\n",
    "    fixedWords = []\n",
    "    for val in words:\n",
    "        newVal = val\n",
    "        if(len(val) == 1 and val.isalpha):\n",
    "            newVal = \"\"\n",
    "        \n",
    "        fixedWords.append(newVal)\n",
    "    \n",
    "    return \" \".join(fixedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0da5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessText(s):\n",
    "    s = s.lower()   #to lower case\n",
    "    s = filterRegex(s) #filter regex in the beginning to remove all weird formats specified in the function\n",
    "    s = removeDigitsFromString(s) # to remove digits inside strings , for ex in enumeration of paragraphs: a0, a1..\n",
    "    s = decontract_words(s)   #to remove decontractions\n",
    "    s = remove_stopwords(s)    \n",
    "    s = remove_alone_char(s) # to remove enumeration of paragraphs with letters:\n",
    "                                #a, b and also remove the remaining string from the function removeDigitsFromString\n",
    "    s = remove_nonsense(s)  # removes any word that doesnt not make sense\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b03384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessFile(filePath):\n",
    "    docToChange = Document(filePath)\n",
    "    for paragraph in docToChange.paragraphs:\n",
    "         paragraph.text = preprocessText(paragraph.text)\n",
    "    docToChange.save(filePath)\n",
    "\n",
    "\n",
    "def preprocessDirectory(directoryName):\n",
    "    for file in os.listdir(\"./\" + directoryName):\n",
    "        if  file.endswith(\".docx\"):\n",
    "            print(\"preprocessing \"+file +\" ...\")\n",
    "            preprocessFile(\"./\" + directoryName + \"/\"+file)\n",
    "    print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f1f1fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing SDG1.docx ...\n",
      "preprocessing SDG10.docx ...\n",
      "preprocessing SDG11.docx ...\n",
      "preprocessing SDG12.docx ...\n",
      "preprocessing SDG13.docx ...\n",
      "preprocessing SDG14.docx ...\n",
      "preprocessing SDG15.docx ...\n",
      "preprocessing SDG16.docx ...\n",
      "preprocessing SDG17.docx ...\n",
      "preprocessing SDG2.docx ...\n",
      "preprocessing SDG3.docx ...\n",
      "preprocessing SDG4.docx ...\n",
      "preprocessing SDG5.docx ...\n",
      "preprocessing SDG6.docx ...\n",
      "preprocessing SDG7.docx ...\n",
      "preprocessing SDG8.docx ...\n",
      "preprocessing SDG9.docx ...\n",
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "preprocessDirectory(\"corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c972aef",
   "metadata": {},
   "source": [
    "# Vectorization using GloVe\n",
    "we will use GloVe, which is an already read set of mappings between words and their vectors. These mappings were created based on millions of words using unsupervised learning. We will use glove which has each word and its corresponding vector in order to be able to generate an array for the words in our document. At the end, we will need to transform this array of vectors into one vector only for each SDG document.\n",
    "links used for glove:\n",
    "https://analyticsindiamag.com/hands-on-guide-to-word-embeddings-using-glove/\n",
    "https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db#:~:text=Brief%20Introduction%20to%20GloVe,in%20a%20high%2Ddimensional%20space\n",
    "from where to download Glove files that we will use to form our dictionary of glove: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c93e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's import the libraries needed\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b849c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first of all download the glove zipped file that will contains 3 text files, we used the one with approx 800 mb\n",
    "#we will use the first text file of 300D (300D is the one with the less dimensions for the vector of each word)\n",
    "\n",
    "#(note that embedding means the vector corresponding to a word)\n",
    "#now let's read the text file that we will use to get our pre trained embeddings\n",
    "\n",
    "\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(\"glove.6B/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8473757f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king',\n",
       " 'queen',\n",
       " 'monarch',\n",
       " 'prince',\n",
       " 'kingdom',\n",
       " 'reign',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'brother',\n",
       " 'crown',\n",
       " 'uncle',\n",
       " 'nephew',\n",
       " 'henry',\n",
       " 'later',\n",
       " 'throne',\n",
       " 'father',\n",
       " 'son',\n",
       " 'succeeded',\n",
       " 'ahrts',\n",
       " 'cousin',\n",
       " 'http://www.co.mo.md.us',\n",
       " ',',\n",
       " 'latter',\n",
       " 'dihg',\n",
       " 'ruler',\n",
       " 'however',\n",
       " 'grandson',\n",
       " 'likewise',\n",
       " 'prohertrib',\n",
       " 'although',\n",
       " '_____________________________________________',\n",
       " 'kings',\n",
       " 'afterwards',\n",
       " 'drohs',\n",
       " 'vi',\n",
       " '.',\n",
       " '65stk',\n",
       " 'instead',\n",
       " 'when',\n",
       " 'bdb94',\n",
       " 'k978-1',\n",
       " 'bulletinyyy',\n",
       " 'str95bb',\n",
       " 'k977-1',\n",
       " 'js94bb',\n",
       " 'indeed',\n",
       " 'http://www.mediabynumbers.com',\n",
       " 'bb96',\n",
       " 'k587-1',\n",
       " 'thus',\n",
       " 'mo95',\n",
       " 'finally',\n",
       " 'piyanart',\n",
       " 'srivalo',\n",
       " 'hahlt',\n",
       " 'mentioned',\n",
       " 'though',\n",
       " 'grandfather',\n",
       " 'tehf',\n",
       " 'appears',\n",
       " 'iv',\n",
       " 'followed',\n",
       " 'brought',\n",
       " 'instance',\n",
       " 'presumably',\n",
       " 'brother-in-law',\n",
       " 'both',\n",
       " 'frederick',\n",
       " 'accompanied',\n",
       " 'fact',\n",
       " '__________________________________',\n",
       " 'edward',\n",
       " 'duke',\n",
       " 'once',\n",
       " 'whom',\n",
       " 'and',\n",
       " 'also',\n",
       " 'named',\n",
       " 'interbk',\n",
       " 'http://www.nwguild.org',\n",
       " 'swahr',\n",
       " 'came',\n",
       " 'albert',\n",
       " 'referred',\n",
       " 'asked',\n",
       " 'after',\n",
       " 'd00l',\n",
       " 'example',\n",
       " 'em96',\n",
       " 'kuhdz',\n",
       " 'kd97',\n",
       " 'legendary',\n",
       " '24aou94',\n",
       " 'convinced',\n",
       " 'wrldcom',\n",
       " 'nor',\n",
       " 'actually',\n",
       " 'furthermore',\n",
       " 'similarly',\n",
       " 'as',\n",
       " 'nevertheless',\n",
       " 'again',\n",
       " 'but',\n",
       " 'naborsind',\n",
       " 'half-brother',\n",
       " 'elizabeth',\n",
       " 'persuaded',\n",
       " 'him',\n",
       " 'neither',\n",
       " 'majesty',\n",
       " 'then',\n",
       " 'even',\n",
       " 'www.slarmy.org',\n",
       " 'addition',\n",
       " 'http://www.oklahomacitynationalmemorial.org',\n",
       " 'svahng',\n",
       " 'supposedly',\n",
       " '—',\n",
       " 'zuraimi',\n",
       " 'rw95',\n",
       " 'hence',\n",
       " 'besides',\n",
       " 'bb94',\n",
       " 'pyoot',\n",
       " 'referring',\n",
       " 'biotechtrst',\n",
       " 'while',\n",
       " 'elder',\n",
       " 'http://www.statoil.com',\n",
       " 'notably',\n",
       " 'ironically',\n",
       " 'great',\n",
       " 'unfortunately',\n",
       " 'turned',\n",
       " 'str94',\n",
       " 'gph04bb',\n",
       " 'jordan',\n",
       " 'time',\n",
       " 'tahsh',\n",
       " 'soon',\n",
       " 'present',\n",
       " 'whereupon',\n",
       " 'js04bb',\n",
       " 'greg.wilcoxdailynews.com',\n",
       " 'well',\n",
       " 'occasion',\n",
       " 'called',\n",
       " 'nasdaq100tr',\n",
       " 'supposed',\n",
       " 'regent',\n",
       " 'moreover',\n",
       " 'taken',\n",
       " 'perhaps',\n",
       " 'of',\n",
       " 'eventually',\n",
       " 'philip',\n",
       " 'consequently',\n",
       " 'royal',\n",
       " 'himself',\n",
       " 'unlike',\n",
       " 'took',\n",
       " 'eldest',\n",
       " 'devonengy',\n",
       " 'same',\n",
       " 'comes',\n",
       " 'wuhr',\n",
       " 'the',\n",
       " 'nokiacorp',\n",
       " 'chose',\n",
       " 'either',\n",
       " 'having',\n",
       " 'presented',\n",
       " 'part',\n",
       " 'preceded',\n",
       " 'hnkj',\n",
       " 'given',\n",
       " '____________________________________________',\n",
       " 'vwahr',\n",
       " 'luther',\n",
       " 'kroyts',\n",
       " 'nasdaq100',\n",
       " 'charles',\n",
       " 'wanted',\n",
       " 'only',\n",
       " 'son-in-law',\n",
       " 'saw',\n",
       " 'hahj',\n",
       " 'first',\n",
       " 'returned',\n",
       " 'vii',\n",
       " 'possibly',\n",
       " 'met',\n",
       " 'before',\n",
       " 'probably',\n",
       " 'was',\n",
       " 'meanwhile',\n",
       " 'legend',\n",
       " 'return',\n",
       " 'decided',\n",
       " 'ruhv',\n",
       " 'husband',\n",
       " 'rather',\n",
       " 'zheed',\n",
       " 'father-in-law',\n",
       " 'gave',\n",
       " 'behest',\n",
       " 'baldwin',\n",
       " 'gwahng',\n",
       " 'subsequently',\n",
       " 'this',\n",
       " 'ooooooooooooooooooooooooooooooooooooooo',\n",
       " 'turn',\n",
       " 'aka',\n",
       " 'therefore',\n",
       " 'grahts',\n",
       " 'sons',\n",
       " 'alongside',\n",
       " 'shortly',\n",
       " 'meantime',\n",
       " 'recently',\n",
       " 'lion',\n",
       " 'wc2003-sri',\n",
       " 'william',\n",
       " 'muhlt',\n",
       " 'js03',\n",
       " 'spoolname',\n",
       " 'meant',\n",
       " 'come',\n",
       " 'thahl',\n",
       " 'one',\n",
       " 'mention',\n",
       " 'exception',\n",
       " 'nangk',\n",
       " 'initially',\n",
       " 'invited',\n",
       " 'explained',\n",
       " 'reminded',\n",
       " 'asking',\n",
       " 'hoond',\n",
       " 'means',\n",
       " 'because',\n",
       " 'last',\n",
       " 'tihg',\n",
       " 'genlelec',\n",
       " 'ago',\n",
       " 'leaving',\n",
       " 'so',\n",
       " 'made',\n",
       " 'upon',\n",
       " 'suggested',\n",
       " 'crowned',\n",
       " 'famous',\n",
       " 'became',\n",
       " 'way',\n",
       " 'successor',\n",
       " 'he',\n",
       " 'flashaafp.com',\n",
       " 'beginning',\n",
       " 'rohch',\n",
       " 'chosen',\n",
       " 'wished',\n",
       " '30-270',\n",
       " 'marykane2000',\n",
       " 'immediately',\n",
       " 'nyahn',\n",
       " 'aforementioned',\n",
       " 'noting',\n",
       " 'wife',\n",
       " 'nortelnet',\n",
       " 'nonetheless',\n",
       " 'which',\n",
       " 'others',\n",
       " 'whereas',\n",
       " 'afterward',\n",
       " 'following',\n",
       " 'rulers',\n",
       " 'shohr',\n",
       " 'another',\n",
       " 'apparently',\n",
       " 'where',\n",
       " 'george',\n",
       " 'like',\n",
       " 'jawng',\n",
       " 'niece',\n",
       " 'sihp',\n",
       " 'ultimately',\n",
       " 'refer',\n",
       " 'rohsh',\n",
       " 'originally',\n",
       " 'dellcptr',\n",
       " 'zheev',\n",
       " 'today',\n",
       " 'nothing',\n",
       " 'giving',\n",
       " 'tehz',\n",
       " 'kd94',\n",
       " 'wc2003-wis',\n",
       " 'thought',\n",
       " 'included',\n",
       " 'challenged',\n",
       " 'remembered',\n",
       " 'except',\n",
       " 'arrival',\n",
       " 'who',\n",
       " 'friend',\n",
       " 'brohd',\n",
       " 'noted',\n",
       " 'chihn',\n",
       " 'tells',\n",
       " 'did',\n",
       " 'nyev',\n",
       " 'went',\n",
       " 'stated',\n",
       " 'man',\n",
       " 'walker',\n",
       " 'explains',\n",
       " 'known',\n",
       " 'returning',\n",
       " 'succession',\n",
       " 'heir',\n",
       " 'grij',\n",
       " 'seeing',\n",
       " '”',\n",
       " 'banished',\n",
       " 'earlier',\n",
       " 'why',\n",
       " 'refused',\n",
       " 'behalf',\n",
       " 'calling',\n",
       " 'tried',\n",
       " 'nytfileitnotes.ap.org',\n",
       " 'established',\n",
       " 'reason',\n",
       " 'mentions',\n",
       " 'remarked',\n",
       " 'already',\n",
       " 'whenever',\n",
       " 'lehf',\n",
       " 'now',\n",
       " 'hohtz',\n",
       " 'bohld',\n",
       " 'name',\n",
       " 'dehz',\n",
       " 'inspired',\n",
       " 'nobles',\n",
       " 'seems',\n",
       " 'especially',\n",
       " 'that',\n",
       " 'rule',\n",
       " 'never',\n",
       " 'nuhv',\n",
       " 'endtag',\n",
       " 'put',\n",
       " 'zhadh',\n",
       " 'whose',\n",
       " 'dwayk',\n",
       " 'prior',\n",
       " 'fruj',\n",
       " '[',\n",
       " 'back',\n",
       " 'tyahn',\n",
       " 'approached',\n",
       " 'mother',\n",
       " 'rehz',\n",
       " 'personally',\n",
       " 'dubbed',\n",
       " 'burger',\n",
       " 'princess',\n",
       " 'suggestion',\n",
       " 'insisted',\n",
       " 'ask',\n",
       " 'certainly',\n",
       " 'telling',\n",
       " 'bruhth',\n",
       " 'summoned',\n",
       " 'informed',\n",
       " 'whilst',\n",
       " 'http://www.opel.com',\n",
       " 'successors',\n",
       " 'wish',\n",
       " 'tribute',\n",
       " 'assumed',\n",
       " 'granted',\n",
       " 'namely',\n",
       " 'knight',\n",
       " 'suggests',\n",
       " 'grihn',\n",
       " 'tuhv',\n",
       " 'daughter',\n",
       " 'coming',\n",
       " 'yet',\n",
       " 'tvtonightnytsyn.com',\n",
       " 'likes',\n",
       " 'death',\n",
       " 'seen',\n",
       " '’s',\n",
       " 'day',\n",
       " '206-448-8135',\n",
       " 'hoped',\n",
       " 'chayng',\n",
       " 'sent',\n",
       " 'here',\n",
       " 'rest',\n",
       " 'claiming',\n",
       " 'coronation',\n",
       " 'david.lazarus@latimes.com',\n",
       " 'accepted',\n",
       " 'always',\n",
       " 'promised',\n",
       " 'turning',\n",
       " 'led',\n",
       " 'give',\n",
       " 'bringing',\n",
       " 'tell',\n",
       " 'nihz',\n",
       " 'appeared',\n",
       " 'wehd',\n",
       " 'pyahng',\n",
       " 'shaw',\n",
       " 'kd96',\n",
       " '“',\n",
       " 'namesake',\n",
       " 'ruled',\n",
       " 'likened',\n",
       " \"'s\",\n",
       " 'myehrsh',\n",
       " 'lastly',\n",
       " '#ukqec',\n",
       " 'zhohn',\n",
       " 'oly-2004-tennis',\n",
       " 'granddaughter',\n",
       " 'realm',\n",
       " 'offered',\n",
       " 'descendant',\n",
       " 'regarded',\n",
       " 'believing',\n",
       " 'honor',\n",
       " 'trahlt',\n",
       " 'future',\n",
       " 'sought',\n",
       " 'additionally',\n",
       " 'believed',\n",
       " 'absence',\n",
       " 'his',\n",
       " 'take',\n",
       " 'follow',\n",
       " 'chehn',\n",
       " '0080ff',\n",
       " 'in',\n",
       " 'vursh',\n",
       " 'what',\n",
       " 'harald',\n",
       " 'deceased',\n",
       " 'oly-2004-gymnastics',\n",
       " 'proclaimed',\n",
       " 'requested',\n",
       " 'warrior',\n",
       " 'returns',\n",
       " 'oly-2004-box',\n",
       " 'oly-2004-fhockey',\n",
       " 'able',\n",
       " 'created',\n",
       " 'night',\n",
       " 'own',\n",
       " 'commented',\n",
       " 'www.caib.us',\n",
       " 'prihn',\n",
       " 'chahs',\n",
       " 'murphy',\n",
       " 'subsequent',\n",
       " 'kahrd',\n",
       " 'second',\n",
       " 'taking',\n",
       " 'talking',\n",
       " 'princes',\n",
       " 'timewrn',\n",
       " 'being',\n",
       " 'wake',\n",
       " 'reportedly',\n",
       " 'bb97',\n",
       " 'see',\n",
       " 'palace',\n",
       " 'encouraged',\n",
       " 'stehf',\n",
       " '?',\n",
       " 'kcfw2.kcstar.com',\n",
       " 'next',\n",
       " 'describing',\n",
       " 'good',\n",
       " 'replaced',\n",
       " 'fffxf',\n",
       " 'call',\n",
       " 'deposed',\n",
       " 'merely',\n",
       " 'homage',\n",
       " 'with',\n",
       " 'believes',\n",
       " 'shuhn',\n",
       " 'lihb',\n",
       " 'mcdonald',\n",
       " 'rehts',\n",
       " 'according',\n",
       " 'result',\n",
       " 'acknowledged',\n",
       " 'jehf',\n",
       " 'not',\n",
       " 'oly-2004-weightlifting',\n",
       " 'find',\n",
       " 'anyway',\n",
       " 'pawf',\n",
       " 'http://www.nifc.gov/',\n",
       " 'alfonso',\n",
       " 'it',\n",
       " 'nethrlnds',\n",
       " 'ihj',\n",
       " 'simply',\n",
       " 'true',\n",
       " 'visited',\n",
       " 'fahnt',\n",
       " 'courtesy',\n",
       " 'ffxf',\n",
       " 'request',\n",
       " 'miller',\n",
       " 'asks',\n",
       " 'interestingly',\n",
       " 'considered',\n",
       " 'days',\n",
       " 'constantine',\n",
       " 'beloved',\n",
       " 'goes',\n",
       " 'oly-2004-cycling',\n",
       " 'wise',\n",
       " 'arrived',\n",
       " 'sister',\n",
       " 'impressed',\n",
       " 'thanks',\n",
       " 'described',\n",
       " 'evidently',\n",
       " 'kind',\n",
       " 'spoke',\n",
       " 'realized',\n",
       " 'explaining',\n",
       " 'myehs',\n",
       " 'gives',\n",
       " 'tihn',\n",
       " 'thruh',\n",
       " 'mr.',\n",
       " 'asserted',\n",
       " 'portrayed',\n",
       " 'intended',\n",
       " 'honour',\n",
       " '28aou94',\n",
       " 'hand',\n",
       " 'alone',\n",
       " 'vahlz',\n",
       " 'continued',\n",
       " 'lihks',\n",
       " 'indnsia',\n",
       " 'rewarded',\n",
       " 'i.e.',\n",
       " 'sort',\n",
       " 'wilson',\n",
       " 'janlgardner@yahoo.com',\n",
       " 'surprised',\n",
       " 'shroh',\n",
       " 'appear',\n",
       " 'confronted',\n",
       " 'bring',\n",
       " 'would',\n",
       " 'making',\n",
       " 'stating',\n",
       " 'is',\n",
       " '.71098',\n",
       " 'saying',\n",
       " 'recognized',\n",
       " 'such',\n",
       " 'thihr',\n",
       " 'passed',\n",
       " 'thinks',\n",
       " 'tissottiming.com',\n",
       " 'viii',\n",
       " 'gone',\n",
       " 'kohsh',\n",
       " 'kd95',\n",
       " 'suggesting',\n",
       " 'a.k.a.',\n",
       " 'colleague',\n",
       " 'sancho',\n",
       " 'leave',\n",
       " 'krumv',\n",
       " 'etc.',\n",
       " 'spite',\n",
       " 'week',\n",
       " 'richard',\n",
       " 'hahmz',\n",
       " 'forced',\n",
       " 'few',\n",
       " 'widow',\n",
       " 'isabella',\n",
       " 'myoot',\n",
       " 'dj025b',\n",
       " 'supported',\n",
       " 'baker',\n",
       " '722-0192',\n",
       " 'all',\n",
       " 'replied',\n",
       " 'runs_pujols',\n",
       " 'so-called',\n",
       " 'rohvsk',\n",
       " 'hoping',\n",
       " 'parker',\n",
       " 'along',\n",
       " 'jeongjo',\n",
       " 'james',\n",
       " 'jehp',\n",
       " 'shmahn',\n",
       " 'unable',\n",
       " 'previously',\n",
       " 'respectively',\n",
       " 'vega@globe.com',\n",
       " 'happy',\n",
       " 'describes',\n",
       " 'talk',\n",
       " 'joined',\n",
       " 'vrih',\n",
       " 'end',\n",
       " 'eric',\n",
       " 'sultan',\n",
       " 'appointed',\n",
       " 'wrote',\n",
       " 'takes',\n",
       " 'leopold',\n",
       " 'recalled',\n",
       " 'shehp',\n",
       " 'marry',\n",
       " 'nohv',\n",
       " 'headed',\n",
       " 'evidenced',\n",
       " 'brnws',\n",
       " 'calls',\n",
       " 'wherein',\n",
       " 'hahrd',\n",
       " 'reigned',\n",
       " 'coincidentally',\n",
       " 'creation',\n",
       " 'order',\n",
       " 'acted',\n",
       " 'changed',\n",
       " 'hyeonjong',\n",
       " 'initiated',\n",
       " 'thereafter',\n",
       " 'davis',\n",
       " 'doubt',\n",
       " 'implying',\n",
       " 'klehb',\n",
       " 'concerned',\n",
       " 'welcomed',\n",
       " 'may',\n",
       " 'fffx',\n",
       " 'importantly',\n",
       " 'oxeant',\n",
       " 'francis',\n",
       " 'make',\n",
       " 'hope',\n",
       " 'including',\n",
       " 'they',\n",
       " 'might',\n",
       " 'sloht',\n",
       " 'dashst',\n",
       " 'been',\n",
       " 'succeeding',\n",
       " 'vajiravudh',\n",
       " 'yes',\n",
       " 'couple',\n",
       " 'had',\n",
       " 'lear',\n",
       " 'predecessor',\n",
       " 'similar',\n",
       " 'wants',\n",
       " '1973',\n",
       " 'putting',\n",
       " 'tāwhiao',\n",
       " 'appointment',\n",
       " 'venzuel',\n",
       " 'dutugemunu',\n",
       " 'contrast',\n",
       " 'pleased',\n",
       " 'presence',\n",
       " 'long',\n",
       " 'djecr',\n",
       " 'establishment',\n",
       " 'wishes',\n",
       " 'gweye',\n",
       " 'succeed',\n",
       " 'hugh',\n",
       " 'introduced',\n",
       " 'favour',\n",
       " 'regard',\n",
       " 'everyone',\n",
       " 'hero',\n",
       " 'abdullah',\n",
       " 'adding',\n",
       " 'to',\n",
       " 'revived',\n",
       " 'dj025u',\n",
       " '518-623-2825',\n",
       " 'thohr',\n",
       " 'several',\n",
       " 'ever',\n",
       " 'appearing',\n",
       " 'instructed',\n",
       " 'entered',\n",
       " 'refers',\n",
       " 'none',\n",
       " '‘',\n",
       " 'idea',\n",
       " 'believe',\n",
       " 'whether',\n",
       " 'by',\n",
       " 'strahk',\n",
       " 'ordered',\n",
       " 'incidentally',\n",
       " 'just',\n",
       " 'stahf',\n",
       " 'undoubtedly',\n",
       " 'nihk',\n",
       " 'years',\n",
       " 'written',\n",
       " 'newly',\n",
       " 'determined',\n",
       " 'much',\n",
       " 'nahkt',\n",
       " 'go',\n",
       " 'disambiguation',\n",
       " 'likely',\n",
       " 'them',\n",
       " 'middle',\n",
       " 'wc2003-pak',\n",
       " 'marcyniuk',\n",
       " 'holland',\n",
       " 'stephen',\n",
       " 'assured',\n",
       " 'lg03',\n",
       " 'descended',\n",
       " 'prepared',\n",
       " 'zh00r',\n",
       " 'done',\n",
       " 'nelson',\n",
       " 'resulted',\n",
       " 'despite',\n",
       " 'great-grandson',\n",
       " 'meaning',\n",
       " 'margaret',\n",
       " 'without',\n",
       " 'knows',\n",
       " 'wonder',\n",
       " 'ehsh',\n",
       " 'zehs',\n",
       " 'tradition',\n",
       " 'vahld',\n",
       " 'morrison',\n",
       " 'countered',\n",
       " 'remember',\n",
       " \"doo'ah\",\n",
       " '...',\n",
       " 'does',\n",
       " 'third',\n",
       " 'jmccabe@globe.com',\n",
       " 'standing',\n",
       " 'vruhl',\n",
       " 'particular',\n",
       " 'howard',\n",
       " 'delivered',\n",
       " 'martin',\n",
       " 'reveals',\n",
       " 'reminds',\n",
       " 'wanting',\n",
       " 'declaring',\n",
       " 'quest',\n",
       " 'suht',\n",
       " 'conjunction',\n",
       " 'noonz',\n",
       " 'talked',\n",
       " 'replacing',\n",
       " 'choice',\n",
       " 'stayed',\n",
       " 'kahsr',\n",
       " 'waysh',\n",
       " 'should',\n",
       " 'prahnk',\n",
       " 'joining',\n",
       " 'younger',\n",
       " 'during',\n",
       " 'invitation',\n",
       " 'nohrd',\n",
       " 'revealed',\n",
       " 'specifically',\n",
       " 'learned',\n",
       " 'could',\n",
       " 'declared',\n",
       " 'clearly',\n",
       " 'e.g.',\n",
       " 'addressed',\n",
       " 'marrying',\n",
       " 'thereupon',\n",
       " '206-448-8004',\n",
       " 'handed',\n",
       " 'assuming',\n",
       " 'presents',\n",
       " 'lihk',\n",
       " 'year',\n",
       " 'spencer',\n",
       " 'augustus',\n",
       " 'earl',\n",
       " '.70998',\n",
       " 'month',\n",
       " '1978',\n",
       " 'shows',\n",
       " 'brmsg',\n",
       " 'hardly',\n",
       " 'jester',\n",
       " 'fxfx',\n",
       " 'else',\n",
       " 'viewed',\n",
       " '1981',\n",
       " 'little',\n",
       " 'advised',\n",
       " 'sometimes',\n",
       " 'honored',\n",
       " 'if',\n",
       " '1969',\n",
       " 'solomon',\n",
       " 'include',\n",
       " 'anyone',\n",
       " 'thirty',\n",
       " 'patterson',\n",
       " 'appealed',\n",
       " 'scott.collins@latimes.com',\n",
       " 'depicted',\n",
       " 'concluded',\n",
       " 'napoleon',\n",
       " 'shynd',\n",
       " 'decreed',\n",
       " 'maybe',\n",
       " 'welch',\n",
       " 'forget',\n",
       " 'there',\n",
       " 'franks',\n",
       " ']',\n",
       " 'promise',\n",
       " 'patron',\n",
       " 'welcome',\n",
       " 'naming',\n",
       " 'myeongjong',\n",
       " 'outset',\n",
       " 'succeeds',\n",
       " 'relates',\n",
       " 'mahf',\n",
       " 'those',\n",
       " 'concludes',\n",
       " 'stepped',\n",
       " 'briefly',\n",
       " 'still',\n",
       " 'herod',\n",
       " 'agrees',\n",
       " 'frahd',\n",
       " 'consulted',\n",
       " 'localweather',\n",
       " 'knew',\n",
       " 'yr.ago',\n",
       " 'claim',\n",
       " 'transformed',\n",
       " 'x.xx.xx.xx.x',\n",
       " 'presided',\n",
       " 'robinson',\n",
       " 'johnson',\n",
       " 'alumhg',\n",
       " 'late',\n",
       " 'retained',\n",
       " 'march',\n",
       " 'mistress',\n",
       " 'turns',\n",
       " 'agreed',\n",
       " 'zgray',\n",
       " 'argued',\n",
       " 'be',\n",
       " 'enraged',\n",
       " 'consequence',\n",
       " 'mind',\n",
       " 'ooooooooooooooo',\n",
       " 'louis',\n",
       " 'xfff',\n",
       " 'ahbd',\n",
       " 'must',\n",
       " 'gph02',\n",
       " 'has',\n",
       " 'struggle',\n",
       " 'planned',\n",
       " 'twice',\n",
       " '1965',\n",
       " 'alluding',\n",
       " 'arguing',\n",
       " 'shayr',\n",
       " 'b.b.',\n",
       " 'jahch',\n",
       " 'frequently',\n",
       " 'belonged',\n",
       " 'bestowed',\n",
       " 'fortunately',\n",
       " 'phertzberg@nytimes.com',\n",
       " 'huhfs',\n",
       " 'lamented',\n",
       " 'favor',\n",
       " '1975',\n",
       " 'defended',\n",
       " '1968',\n",
       " 'mean',\n",
       " 'title',\n",
       " 'longer',\n",
       " 'these',\n",
       " '1987',\n",
       " 'goujian',\n",
       " 'attempting',\n",
       " 'charlemagne',\n",
       " 'compelled',\n",
       " 'mitchell',\n",
       " 'success',\n",
       " 'asserting',\n",
       " 'portrays',\n",
       " 'xiv',\n",
       " 'accept',\n",
       " 'taejong',\n",
       " 'too',\n",
       " 'particularly',\n",
       " 'evans',\n",
       " 'attempted',\n",
       " 'informs',\n",
       " 'huhnt',\n",
       " 'haakon',\n",
       " 'anything',\n",
       " 'makes',\n",
       " 'becoming',\n",
       " 'confused',\n",
       " 'exactly',\n",
       " 'dragon',\n",
       " 'fxff',\n",
       " 'arthur',\n",
       " 'rbi_mcgwire',\n",
       " '_',\n",
       " 'sister-in-law',\n",
       " 'dk04',\n",
       " 'sure',\n",
       " 'stepping',\n",
       " 'looking',\n",
       " 'head',\n",
       " 'brothers',\n",
       " 'russell',\n",
       " 'seongjong',\n",
       " 'original',\n",
       " 'joohm',\n",
       " 'love',\n",
       " 'choosing',\n",
       " 'early',\n",
       " 'portraying',\n",
       " 'everything',\n",
       " 'other',\n",
       " '…',\n",
       " 'know',\n",
       " 'become',\n",
       " 'turner',\n",
       " 'say',\n",
       " 'suggest',\n",
       " 'join',\n",
       " 'pope',\n",
       " 'out',\n",
       " 'how',\n",
       " 'finds',\n",
       " 'mihz',\n",
       " 'shadow',\n",
       " 'klahk',\n",
       " 'chamberlain',\n",
       " 'ones',\n",
       " '1963',\n",
       " 'bohemia',\n",
       " 'bojang',\n",
       " '74723,3507',\n",
       " 'dv04',\n",
       " 'veht',\n",
       " 'most',\n",
       " 'rsm03',\n",
       " 'unlikely',\n",
       " '1997',\n",
       " 'mutara',\n",
       " 'famously',\n",
       " 'fuhz',\n",
       " 'arranged',\n",
       " 'knowing',\n",
       " 'weeks',\n",
       " 'vidor',\n",
       " 'involved',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#little extra to check how powerful is glove\n",
    "#this function gives us all related words to a word embedding given\n",
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))\n",
    "embeddings_dict\n",
    "\n",
    "find_closest_embeddings(embeddings_dict[\"king\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12b2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to transform our document into an array of vectors, each vector corresponds for a word in that document\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def GloVe(directoryName, fileName):\n",
    "    vectors = []\n",
    "    if  fileName.endswith(\".docx\"):\n",
    "        source_document = Document(\"./\"+ directoryName + \"/\" + fileName)\n",
    "        for paragraph in source_document.paragraphs:\n",
    "            text = paragraph.text\n",
    "            l = word_tokenize(text)\n",
    "            embeddedL = []\n",
    "            for word in l:\n",
    "                if word in embeddings_dict.keys(): #some words may not be in the dictionary, so we check if the word exists in glove first\n",
    "                    embeddedL.append(embeddings_dict[word])\n",
    "            vectors = vectors + embeddedL\n",
    "    \n",
    "    return vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a07b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the list of vectors we got from the previous function and transform into one vector\n",
    "#so instead of having a vector for each word, we will have one vector for the whole document\n",
    "\n",
    "#in this code, we added the vectors and then divided by the number of words in our document\n",
    "#note that the number of words in our document is equal to the number of vectors in the list we have\n",
    "\n",
    "def calculateDocumentVector(listOfVectors):\n",
    "    finalVector = listOfVectors[0]\n",
    "    finalVector = np.subtract(finalVector, listOfVectors[0])\n",
    "    for vector in listOfVectors:\n",
    "        finalVector = np.add(finalVector, vector)\n",
    "    numberOfWordsInDoc = len(listOfVectors)\n",
    "    finalVector = np.divide(finalVector, numberOfWordsInDoc)\n",
    "    return finalVector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff294f6",
   "metadata": {},
   "source": [
    "# Pickle\n",
    "we will be implementing the functions that allow us to store and load the vectors we have in a folder\n",
    "so we will create 2 functions: one to store the vectors for each SDG document and one to load the vector we want later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a67289ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#to learn about pickling https://ianlondon.github.io/blog/pickling-basics/\n",
    "\n",
    "#this function will take the vector and store it as binary file inside the same directory our project is located at\n",
    "#specify wb in order to write it as binary\n",
    "def writePickle(fileName, vectorToStore):\n",
    "   # if(!os.path.exists(\"./\" + directoryName)):\n",
    "   #     os.mkdir(\"./\" + directoryName)\n",
    "    with open(fileName, 'wb') as f:\n",
    "        pickle.dump(vectorToStore, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3635f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will look in the same directory were the project is, and find the file name given and will return its content\n",
    "#specify rb in order to write it as binary\n",
    "\n",
    "def readPickle(fileName):\n",
    "    with open(fileName, 'rb') as f:\n",
    "        readVector = pickle.load(f)\n",
    "    return readVector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c63677",
   "metadata": {},
   "source": [
    "# Creating the vectors of the SDG documents:\n",
    "now is the time to apply all the functions we created in order to generate document vectors for each SDG document in the corpus directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "838d8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that loops over a directory, calculates the vector for each document and saves it as a pickle inside the same directory\n",
    "\n",
    "def creatingVectors(directory):\n",
    "    for file in os.listdir(\"./\"+directory):\n",
    "        if file.endswith(\".docx\"):\n",
    "            f = GloVe(directory,file)\n",
    "            writePickle(file.replace('.docx', '.pickle'),calculateDocumentVector(f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "458e116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the function to calculate all the vectors for each document inside the corpus directory\n",
    "\n",
    "creatingVectors(\"corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798d49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1422a3b2",
   "metadata": {},
   "source": [
    "# Cosine Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbf1714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def calcSimilarity(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd26968",
   "metadata": {},
   "source": [
    "# Testing with new documents\n",
    "put the word you want to test inside the tests file in the same repository and it will be cleaned and you will have the vector inside a pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44ae84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing testingHealth.docx ...\n",
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "preprocessDirectory(\"tests\")\n",
    "creatingVectors(\"tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16663a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166996"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcSimilarity(readPickle(\"SDG3.pickle\"), readPickle(\"testingHealth.pickle\"))\n",
    "#we can see that we will get a high accuracy between the health document and the sdg3 which has a health subject"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
